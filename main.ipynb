{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução e Objetivos\n",
    "\n",
    "Neste projeto, o objetivo é realizar um fine-tuning de um modelo de linguagem pré-treinado utilizando um conjunto de dados JSON (trn.json) contendo informações sobre livros, como título e descrição. O modelo será ajustado para gerar respostas baseadas nas perguntas dos usuários sobre esses produtos, utilizando os dados do dataset como fonte para as respostas.\n",
    "\n",
    "O modelo de linguagem receberá uma pergunta contextualizada pelo título do livro, e a partir disso, fornecerá uma resposta relevante com base no aprendizado obtido durante o fine-tuning. Para avaliar a eficácia do processo, será realizado um teste inicial com o modelo pré-treinado para gerar uma base de comparação antes do ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objetivos\n",
    "\n",
    "1. Receber Perguntas com Contexto do Dataset: O modelo deverá ser capaz de receber perguntas que incluam o título do produto (livro), obtido a partir do arquivo JSON (trn.json), que contém informações do dataset. A pergunta poderá ser algo como: \"O que você pode me dizer sobre o livro [Título]?\"\n",
    "\n",
    "2. Gerar Respostas Baseadas no Fine-Tuning: O modelo, após ser ajustado com o dataset, deverá ser capaz de fornecer respostas baseadas na pergunta feita sobre o título do livro, usando as descrições presentes no dataset como base para gerar respostas.\n",
    "\n",
    "3. Importação do Modelo Pré-Treinado: Será importado um modelo de linguagem de base (como BERT, GPT ou Llama), que será utilizado para o fine-tuning. Um teste será realizado antes de iniciar o treinamento para gerar uma base de análise, comparando os resultados antes e depois do fine-tuning.\n",
    "\n",
    "4. Fine-Tuning do Modelo: O modelo será ajustado utilizando o dataset trn.json preparado, onde o objetivo é melhorar a precisão das respostas geradas pelo modelo, tornando-o capaz de oferecer informações detalhadas e precisas sobre os produtos (livros).\n",
    "\n",
    "5. Teste do Modelo Treinado: Após o treinamento, o modelo será configurado para responder a perguntas dos usuários. O modelo deverá gerar respostas baseadas na pergunta do usuário, utilizando os dados que foram fornecidos durante o processo de fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalar dependências\n",
    "\n",
    "Instala a biblioteca unsloth, que é usada para carregar modelos de linguagem pré-treinados e realizar ajustes finos (fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7kjn6Rr7deU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baixar dataset\n",
    "\n",
    "É feito o download do arquivo comprimido com o dataset LF-Amazon-1.3M, descompacta-o e descomprime um arquivo .gz dentro dele. O arquivo de dados é renomeado para trn.json para facilitar o acesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRtQowTg8lEp"
   },
   "outputs": [],
   "source": [
    "!wget -O \"LF-Amazon-1.3M.zip\" \"https://drive.usercontent.google.com/download?id=12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK&export=download&authuser=0&confirm=t\"\n",
    "!unzip \"LF-Amazon-1.3M.zip\"\n",
    "!gzip --decompress \"./LF-Amazon-1.3M/trn.json.gz\"\n",
    "!mv \"./LF-Amazon-1.3M/trn.json\" \"./trn.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "Aqui, são feitas as importações das bibliotecas necessárias:\n",
    "\n",
    "- `json`: Para trabalhar com arquivos JSON.\n",
    "- `FastLanguageModel` da biblioteca `unsloth`: Para carregar e treinar modelos de linguagem.\n",
    "- `Dataset` da biblioteca `datasets`: Para trabalhar com datasets de maneira eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW2fwj087deV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variávies e constantes\n",
    "\n",
    "Aqui são definidas várias variáveis e constantes:\n",
    "\n",
    "- `max_seq_length`: O comprimento máximo das sequências de texto que serão usadas nos modelos.\n",
    "- `dtype`: O tipo de dado a ser utilizado para a computação (pode ser automático ou otimizado para determinados tipos de hardware).\n",
    "- `load_in_4bit`: Define se a quantização 4bit será usada para reduzir o uso de memória.\n",
    "- `file_path`: O caminho para o arquivo de dados de entrada.\n",
    "- `output_file`: O caminho onde os dados processados serão salvos.\n",
    "- `fourbit_models`: Uma lista de modelos quantizados em 4bit que podem ser usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "file_path = \"trn.json\"\n",
    "output_file = \"pre_processed.json\"\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parâmetros\n",
    "\n",
    "Aqui, são definidos parâmetros para o processamento dos dados:\n",
    "\n",
    "- `LIMIT`: O número máximo de itens a serem processados do dataset.\n",
    "- `IGNORE_EMPTY_VALUES`: Se True, entradas com valores vazios são ignoradas.\n",
    "- `question`: A pergunta usada como entrada para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "LIMIT = 10000  # Limite de itens processados (ajuste conforme necessário)\n",
    "IGNORE_EMPTY_VALUES = True  # Ignorar entradas com valores vazios\n",
    "question = \"What can you tell me about this book?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de uma estrutura de dados (dicionário) para armazenar as instruções, entradas e respostas durante o processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura de saída inicial\n",
    "output_data = {\"instruction\": [], \"input\": [], \"response\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inicialização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processar dados\n",
    "\n",
    "Aqui é feito o processamento do arquivo de dados linha por linha:\n",
    "\n",
    "- Para cada linha, ele carrega um objeto JSON e extrai os campos `title` (título) e `content` (conteúdo).\n",
    "- A instrução (`instruction`) é a mesma para todas as entradas, sendo definida como a pergunta `\"What can you tell me about this book?\"`.\n",
    "- Os dados são armazenados nas listas dentro do dicionário `output_data`, e apenas entradas válidas (não vazias) são mantidas, dependendo do valor de `IGNORE_EMPTY_VALUES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processamento do arquivo\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    count = 0  # Contador de itens processados\n",
    "    for line in file:\n",
    "        if count >= LIMIT:  # Interrompe ao atingir o limite\n",
    "            break\n",
    "        try:\n",
    "            # Carrega o JSON de cada linha\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            instruction = question\n",
    "            title = entry.get(\"title\", \"\").strip()\n",
    "            content = entry.get(\"content\", \"\").strip()\n",
    "\n",
    "            if IGNORE_EMPTY_VALUES:\n",
    "                if instruction and title and content:\n",
    "                    output_data[\"instruction\"].append(instruction)\n",
    "                    output_data[\"input\"].append(title)\n",
    "                    output_data[\"response\"].append(content)\n",
    "            else:\n",
    "                output_data[\"instruction\"].append(instruction)\n",
    "                output_data[\"input\"].append(title)\n",
    "                output_data[\"response\"].append(content)\n",
    "\n",
    "            count += 1\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro ao decodificar linha: {line}\")\n",
    "            print(f\"Detalhes do erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida salva os dados processados no arquivo `pre_processed.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os dados formatados em um arquivo JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, é definido um modelo de prompt (`alpaca_prompt`) que será usado para formatar as entradas para o modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregar e preparar modelo\n",
    "\n",
    "Agora é feito o carregamento de um modelo pré-treinado e o seu tokenizador da biblioteca `unsloth`. O modelo especificado é o `Meta-Llama-3.1-8B`, que é uma versão do modelo Llama em quantização 4bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, é feito o `fine-tuning` do modelo usando parâmetros como `r` (número de camadas LoRA), `lora_alpha` (fator de ajuste de LoRA), entre outros. Este ajuste visa otimizar o modelo para um melhor desempenho com dados específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamos os dados processados previamente para usá-los no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo preprocessado\n",
    "with open(\"pre_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pre_processed_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que formata os dados de entrada (`instruction`, `input` e `response`) para o formato esperado pelo modelo, inserindo o token de fim de sequência (`EOS_TOKEN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqUTGRV37deY"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Preparar os dados em formato adequado\n",
    "dataset_dict = {\n",
    "    \"instruction\": pre_processed_data[\"instruction\"],\n",
    "    \"input\": pre_processed_data[\"input\"],\n",
    "    \"response\": pre_processed_data[\"response\"]\n",
    "}\n",
    "\n",
    "# Criar um dataset a partir do arquivo preprocessado\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Treino\n",
    "\n",
    "Aqui, é criado um treinador (`SFTTrainer`) para o ajuste fino do modelo com os dados processados. O treinador é configurado com parâmetros como número de épocas, taxa de aprendizado, entre outros. Após a configuração, o treinamento é iniciado com o método `.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obuhy7YM7deY"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eFaYkqv7dea"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função permite que você faça perguntas ao modelo, fornecendo uma pergunta e uma entrada. O modelo gera uma resposta com base nesses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twAaBXBx7dea"
   },
   "outputs": [],
   "source": [
    "# Função para perguntas\n",
    "def ask_question(question, input):\n",
    "    # alpaca_prompt = Copied from above\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            question, # instruction\n",
    "            input, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamada da função de perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtQMUUmANQaL"
   },
   "outputs": [],
   "source": [
    "ask_question(\"Who wrote this book?\", \"The Prophet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treinamento, o modelo e o tokenizador ajustados são salvos no diretório lora_model para uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
